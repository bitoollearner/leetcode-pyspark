{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82383caa-9b15-409a-9acb-968b851c477b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83322872-9913-4063-98cf-4aaea5fd3337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e475f002-2b95-4d27-958c-aeab86c066ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1225. Report Contiguous Dates (Hard)**\n",
    "\n",
    "**Table: Failed**\n",
    "\n",
    "| Column Name  | Type    |\n",
    "|--------------|---------|\n",
    "| fail_date    | date    |\n",
    "\n",
    "fail_date is the primary key (column with unique values) for this table.\n",
    "This table contains the days of failed tasks.\n",
    " \n",
    "\n",
    "**Table: Succeeded**\n",
    "\n",
    "| Column Name  | Type    |\n",
    "|--------------|---------|\n",
    "| success_date | date    |\n",
    "\n",
    "success_date is the primary key (column with unique values) for this table.\n",
    "This table contains the days of succeeded tasks.\n",
    " \n",
    "A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\n",
    "\n",
    "**Write a solution to report the period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.**\n",
    "\n",
    "period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\n",
    "\n",
    "Return the result table ordered by start_date.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "**Input:** \n",
    "\n",
    "**Failed table:**\n",
    "\n",
    "| fail_date         |\n",
    "|-------------------|\n",
    "| 2018-12-28        |\n",
    "| 2018-12-29        |\n",
    "| 2019-01-04        |\n",
    "| 2019-01-05        |\n",
    "\n",
    "**Succeeded table:**\n",
    "| success_date      |\n",
    "|-------------------|\n",
    "| 2018-12-30        |\n",
    "| 2018-12-31        |\n",
    "| 2019-01-01        |\n",
    "| 2019-01-02        |\n",
    "| 2019-01-03        |\n",
    "| 2019-01-06        |\n",
    "\n",
    "**Output:** \n",
    "| period_state | start_date   | end_date     |\n",
    "|--------------|--------------|--------------|\n",
    "| succeeded    | 2019-01-01   | 2019-01-03   |\n",
    "| failed       | 2019-01-04   | 2019-01-05   |\n",
    "| succeeded    | 2019-01-06   | 2019-01-06   |\n",
    "\n",
    "**Explanation:** \n",
    "- The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31.\n",
    "- From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was \"succeeded\".\n",
    "- From 2019-01-04 to 2019-01-05 all tasks failed and the system state was \"failed\".\n",
    "- From 2019-01-06 to 2019-01-06 all tasks succeeded and the system state was \"succeeded\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a073d765-b3a4-4bda-a02f-ac5e3671a222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|      date|\n+----------+\n|2018-12-28|\n|2018-12-29|\n|2019-01-04|\n|2019-01-05|\n+----------+\n\n+----------+\n|      date|\n+----------+\n|2018-12-30|\n|2018-12-31|\n|2019-01-01|\n|2019-01-02|\n|2019-01-03|\n|2019-01-06|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "failed_data_1225 = [\n",
    "    (\"2018-12-28\",), (\"2018-12-29\",), (\"2019-01-04\",), (\"2019-01-05\",)\n",
    "]\n",
    "failed_df_1225 = spark.createDataFrame(failed_data_1225, [\"date\"])\n",
    "failed_df_1225.show()\n",
    "\n",
    "succeeded_data_1225 = [\n",
    "    (\"2018-12-30\",), (\"2018-12-31\",), (\"2019-01-01\",), \n",
    "    (\"2019-01-02\",), (\"2019-01-03\",), (\"2019-01-06\",)\n",
    "]\n",
    "\n",
    "succeeded_df_1225 = spark.createDataFrame(succeeded_data_1225, [\"date\"])\n",
    "succeeded_df_1225.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c0dddf3-c766-44d6-84f3-577c068963f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = datetime(2019, 1, 1)\n",
    "end = datetime(2019, 12, 31)\n",
    "calendar = [(start + timedelta(days=i),) for i in range((end - start).days + 1)]\n",
    "calendar_df_1225 = spark.createDataFrame(calendar, [\"dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88fe05f4-ea83-474e-a774-d392c4badc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_df_1225 = failed_df_1225.withColumnRenamed(\"date\", \"fail_date\")\n",
    "succeeded_df_1225 = succeeded_df_1225.withColumnRenamed(\"date\", \"success_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a44ebd7-b2fe-449d-bfdf-fe96f95994c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "annotated_df_1225 = calendar_df_1225 \\\n",
    "    .join(failed_df_1225, calendar_df_1225.dt == failed_df_1225.fail_date, \"left\") \\\n",
    "    .join(succeeded_df_1225, calendar_df_1225.dt == succeeded_df_1225.success_date, \"left\") \\\n",
    "    .withColumn(\"period_state\", when(col(\"fail_date\").isNotNull(), \"failed\").otherwise(\"succeeded\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354e01f6-14e9-41bd-afbc-e16b85f39692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"dt\")\n",
    "\n",
    "annotated_df_1225 = annotated_df_1225\\\n",
    "                            .withColumn(\"prev_state\", lag(\"period_state\").over(window_spec)\n",
    "                                        )\n",
    "\n",
    "annotated_df_1225 = annotated_df_1225\\\n",
    "                            .withColumn(\"is_new_group\",\n",
    "                                when(col(\"prev_state\").isNull() | (col(\"period_state\") != col(\"prev_state\")), lit(1)).otherwise(lit(0))\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440ec081-c233-4faf-99ee-bf64f206d73c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "group_window = Window.orderBy(\"dt\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "annotated_df_1225 = annotated_df_1225\\\n",
    "                            .withColumn(\"group_id\", sum(\"is_new_group\").over(group_window)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "085e9922-e3ff-4956-af21-5a1c3a4f07e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+\n|period_state|         start_date|           end_date|\n+------------+-------------------+-------------------+\n|   succeeded|2019-01-01 00:00:00|2019-01-03 00:00:00|\n|      failed|2019-01-04 00:00:00|2019-01-05 00:00:00|\n|   succeeded|2019-01-06 00:00:00|2019-12-31 00:00:00|\n+------------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "annotated_df_1225\\\n",
    "            .groupBy(\"group_id\", \"period_state\")\\\n",
    "                .agg(\n",
    "                    min(\"dt\").alias(\"start_date\"),\n",
    "                    max(\"dt\").alias(\"end_date\")\n",
    "                )\\\n",
    "                .orderBy(\"start_date\")\\\n",
    "                    .select(\"period_state\", \"start_date\", \"end_date\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1225. Report Contiguous Dates (Hard)-(Solved)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
